---
layout: post
title: Examining Basic Requirements For Mapping in ROS
date: 2018-09-08 09:00:07.000000000 +00:00
type: post
post_id: '16759'
parent_id: '0'
published: true
password: ''
status: publish
categories:
- ROS
tags: []
meta:
  _rest_api_published: '1'
  _rest_api_client_id: "-1"
  _publicize_job_id: '21931500490'
  timeline_notification: '1536422480'
  _elasticsearch_data_sharing_indexed_on: '2024-11-18 13:22:14'
author:
  login: inkarc
  email: luxo.lamp@gmail.com
  display_name: Roger Cheng
  first_name: Roger
  last_name: Cheng
permalink: "/2018/09/08/examining-basic-requirements-for-mapping-in-ros/"
---
<p><!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"><br />
<html><body>
<p>When I <a href="https://newscrewdriver.com/2018/07/30/ros-notes-robot-simulation-with-gazebo/">first looked at Gazebo robot simulation environment</a>, I didn't understand how to find the interface layer difference between a simulated robot and a physical robot. Now I've learned enough ROS to know: <a href="https://newscrewdriver.com/2018/08/08/a-beginners-look-into-the-mind-of-a-simulated-ros-robot/">Look at the ROS Node graph</a> to find a node named <code>/gazebo</code>. Anything provided by or consumed by that node is a virtual substitute provided by Gazebo. When the same ROS code stack is on a physical robot, nodes that interface with physical hardware replaces everything provided in simulation byÂ <code>/gazebo</code>.</p>
<p>When I tested <a href="https://newscrewdriver.com/2018/09/05/learning-when-a-computer-is-struggling-to-perform-ros-mapping/">a cheap little laptop's performance in ROS</a>, I put this knowledge to use. Gazebo ran on a high-end computer and every other node ran on the laptop. This simulates the workload of running <a href="http://wiki.ros.org/gmapping">Gmapping</a> algorithm as if the low-end laptop was mounted on a physical robot. But what, specifically, is required? Let's look at the ROS node graph once again with <a href="http://wiki.ros.org/rqt_graph"><code>rqt_graph</code></a>. Here's a graph generated while <a href="http://emanual.robotis.com/docs/en/platform/turtlebot3/simulation/#virtual-slam-with-turtlebot3">TurtleBot 3's mapping demo</a> is running in Gazebo:</p>
<p><img class=" size-full wp-image-16760 aligncenter" src="https://newscrewdriver.com/wp-content/uploads/2018/09/turtlebot-3-gmapping-with-ui.png" alt="TurtleBot 3 GMapping With UI" width="1030" height="273"></p>
<p>Here's a slightly different graph, generated by running the same mapping task but with Gazebo's GUI and ROS <a href="http://wiki.ros.org/rviz">RViz</a> visualization tool turned off. They are useful for the human developer but are not strictly necessary for the robot to run. We see the <code>/gazebo_gui</code> node has dropped out as expected, and the <code>/map</code> topic was also dropped because it was no longer being consumed by RViz for presentation.</p>
<p><img class="alignnone size-full wp-image-16761" src="https://newscrewdriver.com/wp-content/uploads/2018/09/turtlebot-3-gmapping-no-ui.png" alt="TurtleBot 3 GMapping No UI" width="1591" height="317"></p>
<p>We can see the Gazebo-specific parts are quite modest in this particular exercise. A physical robot running Gmapping in ROS will need to subscribe to <code>/cmd_vel</code> so it can be told where to go, and provide laser distance scanning data via <code>/scan</code> so Gmapping can tell where it is. Gazebo also publishes the simulated robot's state via <code>/tf</code> and <code>/joint_states</code>.</p>
<p>And now that I have a <a href="https://newscrewdriver.com/tag/neato/">spinning LIDAR working in ROS</a> to provide <code>/scan</code>, the next project is to build a robot chassis that can be driven via <code>/cmd_vel</code>. After that is complete, we can use it to learn about <code>/tf</code> and <code>/joint_states</code>.</p>
<p></body></html></p>
