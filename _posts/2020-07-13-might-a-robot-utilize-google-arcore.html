---
layout: post
title: Might A Robot Utilize Google ARCore?
date: 2020-07-13 15:41:00.000000000 +00:00
type: post
post_id: '21833'
parent_id: '0'
published: true
password: ''
status: publish
categories:
- Window Shopping
tags:
- ARCore
meta:
  _thumbnail_id: '21832'
  timeline_notification: '1594680063'
  _rest_api_published: '1'
  _rest_api_client_id: "-1"
  _publicize_job_id: '46537274066'
  _elasticsearch_data_sharing_indexed_on: '2024-11-18 13:27:28'
author:
  login: inkarc
  email: luxo.lamp@gmail.com
  display_name: Roger Cheng
  first_name: Roger
  last_name: Cheng
permalink: "/2020/07/13/might-a-robot-utilize-google-arcore/"
---
<p><!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"><br />
<html><body>
<p>Machine vision is a big field, because there are a lot of useful things we can do when a computer understands what it sees. In narrow machine-friendly niches it has become commonplace, for example the UPC bar code on everyday merchandise is something created for machines to read, and a bar code reader is a very simplified and specific niche of machine vision.</p>
<p>But that is a long, long way from a robot understanding its environment through cameras, with <a href="https://newscrewdriver.com/2020/07/12/i-do-not-yet-meet-the-prerequisites-for-multiple-view-geometry-in-computer-vision/">many sub sections along the path which are entire topics in their own right</a>. Again we have successes in narrow machine-friendly domains such as a factory floor set up for automation. Outside of environments tailored for machines, it gets progressively harder. Roomba and <a href="https://newscrewdriver.com/tag/neato/">similar robot home vacuums like Neato</a> could wander through a human home, but their success depends on a neat and tidy spacious home. As a home becomes more cluttered, success rate of robot vacuums decline.</p>
<p>But they're still using <a href="https://newscrewdriver.com/2018/08/18/neato-vacuum-laser-scanner-works-in-rviz/">specialized sensors</a> and not a camera with vision comparable to human sight. Computers have no problems chugging through a 2D array of pixel data, but extracting useful information is hard. The recent breakthrough in deep learning algorithms opened up more frontiers. The typical example is a classifier, and it's one of the demos that shipped with Google AIY Vision kit. (Though not the default, which was the "Joy Detector.") With a classifier the computer can say "that's a cat" which is a useful step toward something a robot needs, which is more like "there's a house pet in my path and I need to maneuver around it, and I also need to be aware it might get up and move." (This is a very advanced level of thinking for a robot...)</p>
<p>The skill to pick out relevant physical structure from camera image is useful for robots, but not exclusively to robots. Both Google and Apple are building augmented reality (AR) features into phones and tablets. Underlying that feature is some level of ability to determine structure from image, in order to overlay an AR object over the real world. Maybe that capability can be used for a robot? <a href="https://newscrewdriver.com/2020/07/14/first-glance-at-google-arcore-depth-map/">Time for some research</a>.</p>
<p></body></html></p>
