---
layout: post
title: Phoebe The Cartographer
date: 2018-09-20 09:00:18.000000000 +00:00
type: post
post_id: '16803'
parent_id: '0'
published: true
password: ''
status: publish
categories:
- Phoebe TurtleBot
- ROS
tags: []
meta:
  _rest_api_published: '1'
  _rest_api_client_id: "-1"
  timeline_notification: '1537459595'
  _publicize_job_id: '22351254522'
  _elasticsearch_data_sharing_indexed_on: '2024-11-18 13:22:19'
author:
  login: inkarc
  email: luxo.lamp@gmail.com
  display_name: Roger Cheng
  first_name: Roger
  last_name: Cheng
permalink: "/2018/09/20/phoebe-the-cartographer/"
---
<p><!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"><br />
<html><body>
<p>Once odometry calculation math in the <a href="https://github.com/sonyccd/roboclaw_ros">Roboclaw ROS driver</a> was fixed, I could drive Phoebe (my <a href="https://www.turtlebot.com/">TurtleBot</a> variant) around the house and watch laser and odometry data plotted in RViz. It is exciting to see the data stream starting to resemble that of a real functioning autonomous robot! And just like all real robots... the real world does not match the ideal world. Our specific problem of the day is odometry drift: Phoebe's wheel encoders are not perfectly accurate. Whether from wheel slippage, small debris on the ground, or whatever else, they cause the reported distance to be slightly different from actual distance traveled. These small errors accumulate over time, so the position calculated from odometry becomes less and less accurate as Phoebe drives.</p>
<p>The solution to odometry drift is to supplement encoder data with other sensors, using additional information can help correct for position drift. In the case of Phoebe and her  TurtleBot 3 inspiration, that comes in courtesy of the scanning LIDAR. If Phoebe can track LIDAR readings over time and build up a map, that information can also be used to locate Phoebe on the map. This class of algorithms is called SLAM for <a href="https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping">Simultaneous Location and Mapping</a>. And because they're fundamentally similar robots, it would be straightforward to translate <a href="http://emanual.robotis.com/docs/en/platform/turtlebot3/simulation/#virtual-slam-with-turtlebot3">TurtleBot 3's SLAM demo</a> to my Phoebe.</p>
<p>There are several different SLAM implementations available as ROS modules. I'll start with <a href="http://wiki.ros.org/gmapping">Gmapping</a> because that's what TurtleBot 3 demo used. As input this module needs LIDAR data in the form of ROS topic <code>/scan</code> and also the transform tree published via <code>/tf</code>, where it finds the geometry relationship between odometry (which I just fixed), base, and laser. As output, it will generate an "occupancy grid", a big table representing a robot's environment in terms of open space, obstacle, or unknown. And most importantly for our purposes: it will generate a transform mapping <code>map</code> coordinate frame to the <code>odom</code> coordinate frame. This coordinate transform is the correction factor to be applied on top of odometry-calculated position, generated by comparing LIDAR data to the map.</p>
<p>Once all the pieces are in place, Phoebe can start mapping out its environment and also correct for small errors in odometry position as it drifts.</p>
<p>SLAM achievement: Unlocked!</p>
<p><img class="alignnone size-full wp-image-16793" src="https://newscrewdriver.com/wp-content/uploads/2018/09/phoebe-gmapping.png" alt="Phoebe GMapping" width="1465" height="801"></p>
<p>(Cross-posted to <a href="https://hackaday.io/project/161085-phoebe-turtlebot/log/153114-phoebe-the-cartographer">Hackaday.io</a>)</p>
<p></body></html></p>
